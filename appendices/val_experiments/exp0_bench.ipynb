{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation experiments\n",
    "\n",
    "21st  May\n",
    "\n",
    "Linear regression\n",
    "Random Forest\n",
    "SVR with RBF kernel\n",
    "Gaussian Process with RBF kernel\n",
    "Gaussian Procces with Matern kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import gpytorch\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# custom libraries\n",
    "from utils import metrics\n",
    "from models.gpytorch_gp import GPRegressionModel\n",
    "# dir = '/data/hpcdata/users/kenzi22'\n",
    "dir = '/Users/kenzatazi/Documents/CDT/Code'\n",
    "sys.path.append(dir)  # noqa\n",
    "from load import era5, value, data_dir  # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minyear = '2000'\n",
    "maxyear = '2004-12-31'\n",
    "\n",
    "cv_locs = np.load('experiments/exp1/cv/exp1_cv_locs.npy')\n",
    "cv_locs = cv_locs.reshape(-1, 2)\n",
    "\n",
    "gauge_df = value.all_gauge_data(minyear, maxyear, monthly=True)\n",
    "station_names = gauge_df.drop_duplicates('name')['name']\n",
    "\n",
    "station_list = []\n",
    "for loc in cv_locs:\n",
    "    station_row = gauge_df[(gauge_df['lat'] == loc[1]) | (\n",
    "        gauge_df['lon'] == loc[0])].iloc[0]\n",
    "    station_list.append(station_row['name'])\n",
    "station_arr = np.array(station_list)\n",
    "\n",
    "# Split into five chunks\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "cv_train_list = []\n",
    "cv_test_list = []\n",
    "\n",
    "for train_index, test_index in kf.split(station_arr):\n",
    "    [station_names.drop(station_names.loc[station_names == l].index,\n",
    "                        inplace=True) for l in station_arr[test_index]]\n",
    "    hf_train = station_names.values\n",
    "    hf_test = station_arr[test_index]\n",
    "    cv_train_list.append(hf_train)\n",
    "    cv_test_list.append(hf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_all = []\n",
    "RMSE_all = []\n",
    "RMSE_p5 = []\n",
    "RMSE_p95 = []\n",
    "MLL = []\n",
    "\n",
    "for j in range(len(cv_train_list)):\n",
    "\n",
    "    hf_train_list = []\n",
    "    for station in cv_train_list[j]:\n",
    "        station_ds = value.gauge_download(\n",
    "            station, minyear=minyear, maxyear=maxyear)\n",
    "        hf_train_list.append(station_ds.dropna().reset_index())\n",
    "    hf_train_df = pd.concat(hf_train_list)\n",
    "    hf_train_df['time'] = pd.to_datetime(hf_train_df['time'])\n",
    "    hf_train_df['time'] = pd.to_numeric(hf_train_df['time'])\n",
    "\n",
    "    val_list = []\n",
    "    for station in cv_test_list[j]:\n",
    "        station_ds = value.gauge_download(\n",
    "            station, minyear=minyear, maxyear=maxyear)\n",
    "        val_list.append(station_ds.dropna().reset_index())\n",
    "    val_df = pd.concat(val_list)\n",
    "    val_df['time'] = pd.to_datetime(val_df['time'])\n",
    "    val_df['time'] = pd.to_numeric(val_df['time'])\n",
    "\n",
    "    era5_df = era5.value_gauge_download(\n",
    "        list(cv_test_list[j]) + list(cv_train_list[j]), minyear=minyear, maxyear=maxyear)\n",
    "    lf_train_df = era5_df.reset_index()\n",
    "    lf_train_df['time'] = pd.to_datetime(lf_train_df['time'])\n",
    "    lf_train_df['time'] = pd.to_numeric(lf_train_df['time'])\n",
    "\n",
    "    # Prepare data\n",
    "\n",
    "    # Transformations\n",
    "    lf_train_df['tp_tr'], lf_lambda = sp.stats.boxcox(\n",
    "        lf_train_df['tp'].values + 0.01)\n",
    "    hf_train_df['tp_tr'] = sp.stats.boxcox(\n",
    "        hf_train_df['tp'].values + 0.01, lmbda=lf_lambda)\n",
    "    val_df['tp_tr'] = sp.stats.boxcox(\n",
    "        val_df['tp'].values + 0.01, lmbda=lf_lambda)\n",
    "\n",
    "    # Splitting\n",
    "    x_train_hf = hf_train_df[['time', 'lat', 'lon', 'z']].values.reshape(-1, 4)\n",
    "    y_train_hf = hf_train_df[['tp_tr']].values.reshape(-1, 1)\n",
    "    x_val = val_df[['time', 'lat', 'lon', 'z']].values.reshape(-1, 4)\n",
    "    y_val = val_df['tp_tr'].values.reshape(-1, 1)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler().fit(x_train_hf)\n",
    "    x_train_hf1 = scaler.transform(x_train_hf)\n",
    "    x_val1 = scaler.transform(x_val)\n",
    "\n",
    "    # Make tensors\n",
    "    train_x_hf, train_y_hf = torch.Tensor(\n",
    "        x_train_hf1), torch.Tensor(y_train_hf.reshape(-1))\n",
    "\n",
    "    # GP training\n",
    "    likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(\n",
    "        noise=torch.ones(len(train_x_hf)) * 0.01)\n",
    "    model = GPRegressionModel(train_x_hf, train_y_hf, likelihood, kernel='rbf')\n",
    "\n",
    "    training_iter = 200\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    # Includes GaussianLikelihood parameters\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x_hf)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, train_y_hf)\n",
    "        loss.backward()\n",
    "        if i % 10 == 0:\n",
    "            print('Iter %d/%d - Loss: %.3f' % (  # lengthscale: %.3f   noise: %.3f' % (\n",
    "                i + 1, training_iter, loss.item(),\n",
    "                # model.covar_module.base_kernel.lengthscale.item(),\n",
    "                # model.likelihood.noise.item()\n",
    "            ))\n",
    "        optimizer.step()\n",
    "\n",
    "    # Get into evaluation (predictive posterior) mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        trained_pred_dist = likelihood(model(torch.Tensor(x_val1)))\n",
    "        y_pred0 = trained_pred_dist.mean\n",
    "        y_std0 = trained_pred_dist.stddev\n",
    "\n",
    "    y_pred = sp.special.inv_boxcox(y_pred0, lf_lambda).reshape(-1)\n",
    "    y_pred[np.isnan(y_pred)] = 0\n",
    "    y_true = sp.special.inv_boxcox(y_val, lf_lambda).reshape(-1)\n",
    "    R2_all.append(r2_score(y_true, y_pred))\n",
    "    RMSE_all.append(mean_squared_error(y_true, y_pred, squared=False))\n",
    "\n",
    "    # 5th PERCENTILE\n",
    "    p5 = np.percentile(y_true, 5.0)\n",
    "    indx = [y_true <= p5][0]\n",
    "    x_val_p5 = x_val[indx, :]\n",
    "    y_true_p5 = y_true[indx]\n",
    "    y_pred_p5 = y_pred[indx]\n",
    "    RMSE_p5.append(mean_squared_error(y_true_p5, y_pred_p5, squared=False))\n",
    "\n",
    "    # 95th PERCENTILE\n",
    "    p95 = np.percentile(y_true, 95.0)\n",
    "    indx = [y_true >= p95][0]\n",
    "    x_val_p95 = x_val[indx]\n",
    "    y_true_p95 = y_true[indx]\n",
    "    y_pred_p95 = y_pred[indx]\n",
    "    RMSE_p95.append(mean_squared_error(y_true_p95, y_pred_p95, squared=False))\n",
    "\n",
    "    # MLL\n",
    "    ll = metrics.mll(y_val, y_pred0, y_std0)\n",
    "    MLL.append(ll)\n",
    "\n",
    "print('Mean RMSE = ', np.mean(RMSE_all), '±', np.std(RMSE_all))\n",
    "print('Mean R2 = ', np.mean(R2_all), '±', np.std(R2_all))\n",
    "print('5th RMSE = ', np.mean(RMSE_p5), '±', np.std(RMSE_p5))\n",
    "print('95th RMSE = ', np.mean(RMSE_p95), '±', np.std(RMSE_p95))\n",
    "print('MLL= ', np.mean(MLL), '±', np.std(MLL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_all = []\n",
    "RMSE_all = []\n",
    "RMSE_p5 = []\n",
    "RMSE_p95 = []\n",
    "\n",
    "for i in range(len(cv_train_list)):\n",
    "\n",
    "    hf_train_list = []\n",
    "    for station in cv_train_list[i]:\n",
    "        station_ds = value.gauge_download(\n",
    "            station, minyear=minyear, maxyear=maxyear)\n",
    "        hf_train_list.append(station_ds.dropna().reset_index())\n",
    "    hf_train_df = pd.concat(hf_train_list)\n",
    "    hf_train_df['time'] = pd.to_datetime(hf_train_df['time'])\n",
    "    hf_train_df['time'] = pd.to_numeric(hf_train_df['time'])\n",
    "\n",
    "    val_list = []\n",
    "    for station in cv_test_list[i]:\n",
    "        station_ds = value.gauge_download(\n",
    "            station, minyear=minyear, maxyear=maxyear)\n",
    "        val_list.append(station_ds.dropna().reset_index())\n",
    "    val_df = pd.concat(val_list)\n",
    "    val_df['time'] = pd.to_datetime(val_df['time'])\n",
    "    val_df['time'] = pd.to_numeric(val_df['time'])\n",
    "\n",
    "    era5_df = era5.value_gauge_download(\n",
    "        list(cv_test_list[i]) + list(cv_train_list[i]), minyear=minyear, maxyear=maxyear)\n",
    "    lf_train_df = era5_df.reset_index()\n",
    "    lf_train_df['time'] = pd.to_datetime(lf_train_df['time'])\n",
    "    lf_train_df['time'] = pd.to_numeric(lf_train_df['time'])\n",
    "\n",
    "    # Prepare data\n",
    "\n",
    "    # Transformations\n",
    "    lf_train_df['tp_tr'], lf_lambda = sp.stats.boxcox(\n",
    "        lf_train_df['tp'].values + 0.01)\n",
    "    hf_train_df['tp_tr'] = sp.stats.boxcox(\n",
    "        hf_train_df['tp'].values + 0.01, lmbda=lf_lambda)\n",
    "    val_df['tp_tr'] = sp.stats.boxcox(\n",
    "        val_df['tp'].values + 0.01, lmbda=lf_lambda)\n",
    "\n",
    "    # Splitting\n",
    "    x_train_lf = lf_train_df[['time', 'lat', 'lon', 'z']].values.reshape(-1, 4)\n",
    "    y_train_lf = lf_train_df['tp_tr'].values.reshape(-1, 1)\n",
    "    x_train_hf = hf_train_df[['time', 'lat', 'lon', 'z']].values.reshape(-1, 4)\n",
    "    y_train_hf = hf_train_df[['tp_tr']].values.reshape(-1, 1)\n",
    "    x_val = val_df[['time', 'lat', 'lon', 'z']].values.reshape(-1, 4)\n",
    "    y_val = val_df['tp_tr'].values.reshape(-1, 1)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler().fit(x_train_hf)\n",
    "    x_train_hf1 = scaler.transform(x_train_hf)\n",
    "    x_train_lf1 = scaler.transform(x_train_lf)\n",
    "    x_val1 = scaler.transform(x_val)\n",
    "\n",
    "    linear_m = LinearRegression()\n",
    "    linear_m.fit(x_train_lf1, y_train_lf)\n",
    "\n",
    "    # ALL\n",
    "    y_pred = sp.special.inv_boxcox(\n",
    "        linear_m.predict(x_val1), lf_lambda).reshape(-1)\n",
    "    y_true = sp.special.inv_boxcox(y_val, lf_lambda).reshape(-1)\n",
    "    R2_all.append(r2_score(y_true, y_pred))\n",
    "    RMSE_all.append(mean_squared_error(y_true, y_pred, squared=False))\n",
    "\n",
    "    # 5th PERCENTILE\n",
    "    p5 = np.percentile(y_true, 5.0)\n",
    "    indx = [y_true <= p5][0]\n",
    "    x_val_p5 = x_val[indx, :]\n",
    "    y_true_p5 = y_true[indx]\n",
    "    y_pred_p5 = y_pred[indx]\n",
    "    RMSE_p5.append(mean_squared_error(y_true_p5, y_pred_p5, squared=False))\n",
    "\n",
    "    # 95th PERCENTILE\n",
    "    p95 = np.percentile(y_true, 95.0)\n",
    "    indx = [y_true >= p95][0]\n",
    "    x_val_p95 = x_val[indx]\n",
    "    y_true_p95 = y_true[indx]\n",
    "    y_pred_p95 = y_pred[indx]\n",
    "    RMSE_p95.append(mean_squared_error(y_true_p95, y_pred_p95, squared=False))\n",
    "\n",
    "print('Mean RMSE = ', np.mean(RMSE_all), '±', np.std(RMSE_all))\n",
    "print('Mean R2 = ', np.mean(R2_all), '±', np.std(R2_all))\n",
    "print('5th RMSE = ', np.mean(RMSE_p5), '±', np.std(RMSE_p5))\n",
    "print('95th RMSE = ', np.mean(RMSE_p95), '±', np.std(RMSE_p95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_all = []\n",
    "RMSE_all = []\n",
    "RMSE_p5 = []\n",
    "RMSE_p95 = []\n",
    "\n",
    "for i in range(len(cv_train_list)):\n",
    "\n",
    "    hf_train_list = []\n",
    "    for station in cv_train_list[i]:\n",
    "        station_ds = value.gauge_download(\n",
    "            station, minyear=minyear, maxyear=maxyear)\n",
    "        hf_train_list.append(station_ds.dropna().reset_index())\n",
    "    hf_train_df = pd.concat(hf_train_list)\n",
    "    hf_train_df['time'] = pd.to_datetime(hf_train_df['time'])\n",
    "    hf_train_df['time'] = pd.to_numeric(hf_train_df['time'])\n",
    "\n",
    "    val_list = []\n",
    "    for station in cv_test_list[i]:\n",
    "        station_ds = value.gauge_download(\n",
    "            station, minyear=minyear, maxyear=maxyear)\n",
    "        val_list.append(station_ds.dropna().reset_index())\n",
    "    val_df = pd.concat(val_list)\n",
    "    val_df['time'] = pd.to_datetime(val_df['time'])\n",
    "    val_df['time'] = pd.to_numeric(val_df['time'])\n",
    "\n",
    "    era5_df = era5.value_gauge_download(\n",
    "        list(cv_test_list[i]) + list(cv_train_list[i]), minyear=minyear, maxyear=maxyear)\n",
    "    lf_train_df = era5_df.reset_index()\n",
    "    lf_train_df['time'] = pd.to_datetime(lf_train_df['time'])\n",
    "    lf_train_df['time'] = pd.to_numeric(lf_train_df['time'])\n",
    "\n",
    "    # Prepare data\n",
    "\n",
    "    # Transformations\n",
    "    lf_train_df['tp_tr'], lf_lambda = sp.stats.boxcox(\n",
    "        lf_train_df['tp'].values + 0.01)\n",
    "    hf_train_df['tp_tr'] = sp.stats.boxcox(\n",
    "        hf_train_df['tp'].values + 0.01, lmbda=lf_lambda)\n",
    "    val_df['tp_tr'] = sp.stats.boxcox(\n",
    "        val_df['tp'].values + 0.01, lmbda=lf_lambda)\n",
    "\n",
    "    # Splitting\n",
    "    x_train_lf = lf_train_df[['time', 'lat', 'lon', 'z']].values.reshape(-1, 4)\n",
    "    y_train_lf = lf_train_df['tp_tr'].values.reshape(-1, 1)\n",
    "    x_train_hf = hf_train_df[['time', 'lat', 'lon', 'z']].values.reshape(-1, 4)\n",
    "    y_train_hf = hf_train_df[['tp_tr']].values.reshape(-1, 1)\n",
    "    x_val = val_df[['time', 'lat', 'lon', 'z']].values.reshape(-1, 4)\n",
    "    y_val = val_df['tp_tr'].values.reshape(-1, 1)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler().fit(x_train_hf)\n",
    "    x_train_hf1 = scaler.transform(x_train_hf)\n",
    "    x_train_lf1 = scaler.transform(x_train_lf)\n",
    "    x_val1 = scaler.transform(x_val)\n",
    "\n",
    "    rf_m = RandomForestRegressor()\n",
    "    rf_m.fit(x_train_lf1, y_train_lf)\n",
    "\n",
    "    # ALL\n",
    "    y_pred = sp.special.inv_boxcox(\n",
    "        rf_m.predict(x_val1), lf_lambda).reshape(-1)\n",
    "    y_true = sp.special.inv_boxcox(y_val, lf_lambda).reshape(-1)\n",
    "    R2_all.append(r2_score(y_true, y_pred))\n",
    "    RMSE_all.append(mean_squared_error(y_true, y_pred, squared=False))\n",
    "\n",
    "    # 5th PERCENTILE\n",
    "    p5 = np.percentile(y_true, 5.0)\n",
    "    indx = [y_true <= p5][0]\n",
    "    x_val_p5 = x_val[indx, :]\n",
    "    y_true_p5 = y_true[indx]\n",
    "    y_pred_p5 = y_pred[indx]\n",
    "    RMSE_p5.append(mean_squared_error(y_true_p5, y_pred_p5, squared=False))\n",
    "\n",
    "    # 95th PERCENTILE\n",
    "    p95 = np.percentile(y_true, 95.0)\n",
    "    indx = [y_true >= p95][0]\n",
    "    x_val_p95 = x_val[indx]\n",
    "    y_true_p95 = y_true[indx]\n",
    "    y_pred_p95 = y_pred[indx]\n",
    "    RMSE_p95.append(mean_squared_error(y_true_p95, y_pred_p95, squared=False))\n",
    "\n",
    "\n",
    "print('Mean RMSE = ', np.mean(RMSE_all), '±', np.std(RMSE_all))\n",
    "print('Mean R2 = ', np.mean(R2_all), '±', np.std(R2_all))\n",
    "print('5th RMSE = ', np.mean(RMSE_p5), '±', np.std(RMSE_p5))\n",
    "print('95th RMSE = ', np.mean(RMSE_p95), '±', np.std(RMSE_p95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_all = []\n",
    "RMSE_all = []\n",
    "RMSE_p5 = []\n",
    "RMSE_p95 = []\n",
    "\n",
    "for i in range(len(cv_train_list)):\n",
    "\n",
    "    hf_train_list = []\n",
    "    for station in cv_train_list[i]:\n",
    "        station_ds = value.gauge_download(\n",
    "            station, minyear=minyear, maxyear=maxyear)\n",
    "        hf_train_list.append(station_ds.dropna().reset_index())\n",
    "    hf_train_df = pd.concat(hf_train_list)\n",
    "    hf_train_df['time'] = pd.to_datetime(hf_train_df['time'])\n",
    "    hf_train_df['time'] = pd.to_numeric(hf_train_df['time'])\n",
    "\n",
    "    val_list = []\n",
    "    for station in cv_test_list[i]:\n",
    "        station_ds = value.gauge_download(\n",
    "            station, minyear=minyear, maxyear=maxyear)\n",
    "        val_list.append(station_ds.dropna().reset_index())\n",
    "    val_df = pd.concat(val_list)\n",
    "    val_df['time'] = pd.to_datetime(val_df['time'])\n",
    "    val_df['time'] = pd.to_numeric(val_df['time'])\n",
    "\n",
    "    era5_df = era5.value_gauge_download(\n",
    "        list(cv_test_list[i]) + list(cv_train_list[i]), minyear=minyear, maxyear=maxyear)\n",
    "    lf_train_df = era5_df.reset_index()\n",
    "    lf_train_df['time'] = pd.to_datetime(lf_train_df['time'])\n",
    "    lf_train_df['time'] = pd.to_numeric(lf_train_df['time'])\n",
    "\n",
    "    # Prepare data\n",
    "\n",
    "    # Transformations\n",
    "    lf_train_df['tp_tr'], lf_lambda = sp.stats.boxcox(\n",
    "        lf_train_df['tp'].values + 0.01)\n",
    "    hf_train_df['tp_tr'] = sp.stats.boxcox(\n",
    "        hf_train_df['tp'].values + 0.01, lmbda=lf_lambda)\n",
    "    val_df['tp_tr'] = sp.stats.boxcox(\n",
    "        val_df['tp'].values + 0.01, lmbda=lf_lambda)\n",
    "\n",
    "    # Splitting\n",
    "    x_train_lf = lf_train_df[['time', 'lat', 'lon', 'z']].values.reshape(-1, 4)\n",
    "    y_train_lf = lf_train_df['tp_tr'].values.reshape(-1, 1)\n",
    "    x_train_hf = hf_train_df[['time', 'lat', 'lon', 'z']].values.reshape(-1, 4)\n",
    "    y_train_hf = hf_train_df[['tp_tr']].values.reshape(-1, 1)\n",
    "    x_val = val_df[['time', 'lat', 'lon', 'z']].values.reshape(-1, 4)\n",
    "    y_val = val_df['tp_tr'].values.reshape(-1, 1)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler().fit(x_train_hf)\n",
    "    x_train_hf1 = scaler.transform(x_train_hf)\n",
    "    x_train_lf1 = scaler.transform(x_train_lf)\n",
    "    x_val1 = scaler.transform(x_val)\n",
    "\n",
    "    svr_m = SVR()\n",
    "    svr_m.fit(x_train_lf1, y_train_lf)\n",
    "\n",
    "    # ALL\n",
    "    y_pred = sp.special.inv_boxcox(\n",
    "        svr_m.predict(x_val1), lf_lambda).reshape(-1)\n",
    "    y_true = sp.special.inv_boxcox(y_val, lf_lambda).reshape(-1)\n",
    "    R2_all.append(r2_score(y_true, y_pred))\n",
    "    RMSE_all.append(mean_squared_error(y_true, y_pred, squared=False))\n",
    "\n",
    "    # 5th PERCENTILE\n",
    "    p5 = np.percentile(y_true, 5.0)\n",
    "    indx = [y_true <= p5][0]\n",
    "    x_val_p5 = x_val[indx, :]\n",
    "    y_true_p5 = y_true[indx]\n",
    "    y_pred_p5 = y_pred[indx]\n",
    "    RMSE_p5.append(mean_squared_error(y_true_p5, y_pred_p5, squared=False))\n",
    "\n",
    "    # 95th PERCENTILE\n",
    "    p95 = np.percentile(y_true, 95.0)\n",
    "    indx = [y_true >= p95][0]\n",
    "    x_val_p95 = x_val[indx]\n",
    "    y_true_p95 = y_true[indx]\n",
    "    y_pred_p95 = y_pred[indx]\n",
    "    RMSE_p95.append(mean_squared_error(y_true_p95, y_pred_p95, squared=False))\n",
    "\n",
    "\n",
    "print('Mean RMSE = ', np.mean(RMSE_all), '±', np.std(RMSE_all))\n",
    "print('Mean R2 = ', np.mean(R2_all), '±', np.std(R2_all))\n",
    "print('5th RMSE = ', np.mean(RMSE_p5), '±', np.std(RMSE_p5))\n",
    "print('95th RMSE = ', np.mean(RMSE_p95), '±', np.std(RMSE_p95))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
